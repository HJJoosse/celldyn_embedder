{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "#try:\n",
    "#    os.chdir(\"L:\\laupodteam\\AIOS\\Huibert-Jan\\Celldynclustering\\celldyn_embedder\")\n",
    "#except FileNotFoundError:\n",
    "#    os.chdir('C:/Users/Huibert-Jan/Documents/Werk/UMCU/celldyn_embedder')    \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pacmap\n",
    "import umap\n",
    "import trimap\n",
    "\n",
    "#from scipy.stats import chisquare, chi2_contingency, pearsonr\n",
    "#from scipy.stats import kendalltau,spearmanr, weightedtau, theilslopes, wilcoxon, ttest_rel\n",
    "#from scipy.spatial import distance\n",
    "#import dcor\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.manifold import Isomap, MDS, SpectralEmbedding\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE, TSNE, smacof, trustworthiness\n",
    "from sklearn.cluster import KMeans, BisectingKMeans, OPTICS, AffinityPropagation, AgglomerativeClustering, SpectralClustering\n",
    "# add GMM\n",
    "from sklearn.mixture import GaussianMixture as GMM, BayesianGaussianMixture as BGMM\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#import skfuzzy as fuzz\n",
    "\n",
    "\n",
    "#from sklearn.metrics import rand_score, adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "#from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "from hembedder.utils.distance import poincarre_dist, hyperboloid_dist, fractional_distance, Distance\n",
    "from hembedder.utils.quality_metrics import CDEmbeddingPerformance\n",
    "#import numpy.linalg as la\n",
    "#import torch \n",
    "\n",
    "#from numba import njit\n",
    "\n",
    "#import faiss\n",
    "import gc\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pysr import PySRRegressor\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "#from sklego.meta import ZeroInflatedRegressor\n",
    "#from lineartree import LinearTreeRegressor, LinearForestRegressor\n",
    "\n",
    "from xgboost import XGBRegressor, XGBRFClassifier\n",
    "\n",
    "from typing import List, Tuple, Iterable, Callable, Literal\n",
    "\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take class BGMM and extend with a function called bic\n",
    "class BGMM(BGMM):\n",
    "    def _n_parameters(self, X):\n",
    "          \"\"\"Return the number of free parameters in the model.\"\"\"\n",
    "          _, n_features = self.means_.shape\n",
    "          # Number of effective components equals the number of unique labels\n",
    "          n_effect_comp = len(np.unique(self.predict(X)))\n",
    "          #n_effect_comp = self.n_components\n",
    "          if self.covariance_type == 'full':\n",
    "              cov_params = n_effect_comp * n_features * (n_features + 1) / 2.\n",
    "          elif self.covariance_type == 'diag':\n",
    "              cov_params = n_effect_comp * n_features\n",
    "          elif self.covariance_type == 'tied':\n",
    "              cov_params = n_features * (n_features + 1) / 2.\n",
    "          elif self.covariance_type == 'spherical':\n",
    "              cov_params = n_effect_comp\n",
    "          mean_params = n_features * n_effect_comp\n",
    "          return int(cov_params + mean_params + n_effect_comp - 1)\n",
    "\n",
    "    def bic(self, X):\n",
    "        \"\"\"Bayesian information criterion for the current model on the input X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape (n_samples, n_dimensions)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bic : float\n",
    "            The lower the better.\n",
    "        \"\"\"\n",
    "        return (-2 * self.score(X) * X.shape[0] + self._n_parameters(X) * np.log(X.shape[0]))\n",
    "\n",
    "    def aic(self, X):\n",
    "        \"\"\"Akaike information criterion for the current model on the input X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape (n_samples, n_dimensions)\n",
    "+\n",
    "        Returns\n",
    "        -------\n",
    "        aic : float\n",
    "            The lower the better.\n",
    "        \"\"\"\n",
    "        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_classification_f1(X, labels):\n",
    "    '''\n",
    "        create a logistic regression classifier to predict cluster labels\n",
    "        , determine the stratified cross-validated F1 score of the model.\n",
    "    '''\n",
    "    # create a logistic regression classifier to predict cluster labels\n",
    "    clf = LogisticRegression(random_state=0, penalty='elasticnet', solver='saga', l1_ratio=0.1,\n",
    "                                multi_class='multinomial', max_iter=5000)\n",
    "\n",
    "    # determine the stratified cross-validated F1 score of the model.\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in skf.split(X, labels):\n",
    "        clf.fit(X[train_index], labels[train_index])\n",
    "        f1_scores.append(f1_score(labels[test_index], clf.predict(X[test_index]), average='weighted'))\n",
    "\n",
    "    return np.mean(f1_scores), np.std(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to run a clustering algorithm in a bootstrapping fashion\n",
    "# the outcome is a list of clustering models\n",
    "def clustering_bootstrapping(data: pd.DataFrame, \n",
    "                             n_clusters: int=10,\n",
    "                             n_bootstraps: int=100, \n",
    "                             sample_size: int=50000,\n",
    "                             clusterer: str='kmeans',\n",
    "                             align: bool=False,\n",
    "                             **kwargs)-> List[Callable]:\n",
    "    if kwargs.get('n_init', None) is None:\n",
    "        n_init = 10\n",
    "    else:\n",
    "        n_init = kwargs.get('n_init')\n",
    "    \n",
    "    model_list = []\n",
    "    for i in tqdm(range(n_bootstraps)):\n",
    "        # sample data\n",
    "        sample = data.sample(sample_size, replace=True)\n",
    "        # cluster sample\n",
    "        if clusterer == 'kmeans':\n",
    "            if (i==0) | (align==False):\n",
    "                model = KMeans(n_clusters=n_clusters, random_state=123, n_init=n_init)\n",
    "                model.fit(sample)\n",
    "                init_means = model.cluster_centers_\n",
    "            else:\n",
    "                model = KMeans(n_clusters=n_clusters, random_state=123, init=init_means, n_init=n_init)\n",
    "                model.fit(sample)\n",
    "        elif clusterer == 'optics':\n",
    "            model = OPTICS(min_samples=100, n_jobs=-1)\n",
    "            model.fit(sample)\n",
    "        elif clusterer == 'hierarchical':\n",
    "            model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "            model.fit(sample)\n",
    "        elif clusterer == 'gmm':\n",
    "            if (i==0) | (align==False):\n",
    "                model = GMM(n_components=n_clusters, random_state=123)\n",
    "                model.fit(sample)\n",
    "                init_means = model.means_\n",
    "            else:\n",
    "                model = GMM(n_components=n_clusters, random_state=123, means_init=init_means)\n",
    "                model.fit(sample)\n",
    "        elif clusterer == 'bgmm':\n",
    "            if (i==0) | (align==False):\n",
    "                model = BGMM(n_components=n_clusters, random_state=123, max_iter=500)\n",
    "                model.fit(sample)\n",
    "                init_means = model.mean_prior_\n",
    "            else:\n",
    "                model = BGMM(n_components=n_clusters, random_state=123, mean_prior=init_means, max_iter=500)\n",
    "                model.fit(sample)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown clustering algorithm\")\n",
    "        model_list.append(model)\n",
    "    return model_list\n",
    "    \n",
    "def assign_clusters(data: pd.DataFrame, \n",
    "                    model_list: List[Callable], \n",
    "                    clusterer='kmeans') -> pd.DataFrame:\n",
    "    \n",
    "    # get cluster labels\n",
    "    if clusterer in ['kmeans']:\n",
    "        labels = np.array([model.predict(data) for model in model_list])\n",
    "        # get cluster centers\n",
    "        centers = np.array([model.cluster_centers_ for model in model_list])\n",
    "        # get cluster sizes\n",
    "        sizes = np.array([np.bincount(label, minlength=centers.shape[1]) for label in labels])\n",
    "    elif clusterer in ['gmm', 'bgmm']:\n",
    "        labels = np.array([model.predict(data) for model in model_list])\n",
    "        # get cluster centers\n",
    "        centers = np.array([model.means_ for model in model_list])\n",
    "        # get cluster sizes\n",
    "        sizes = np.array([np.bincount(label, minlength=centers.shape[1]) for label in labels])        \n",
    "    return labels, centers, sizes \n",
    "\n",
    "\n",
    "def distance(v1, v2, metric='euclidean'):\n",
    "    ''' Calculate the distance between two vectors '''\n",
    "    if metric=='euclidean':\n",
    "        return np.linalg.norm(v1 - v2)\n",
    "    elif metric=='cosine':\n",
    "        return 1 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    elif metric=='poincarre':\n",
    "        return poincarre_dist(v1, v2)\n",
    "    elif metric=='manhattan':\n",
    "        return np.linalg.norm(v1 - v2, ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_path = 'L:/laupodteam/AIOS/Bram/data/CellDyn/artifacts/celldyn_FULL_transformed_df_updated.feather'\n",
    "celldyn_full = pd.read_feather(cd_path)\n",
    "\n",
    "meas_columns = [c for c in celldyn_full.columns if ('c_b' in c) | (\"COMBO\" in c)]\n",
    "mode_columns = [c for c in celldyn_full.columns if 'c_m' in c]\n",
    "alrt_columns = [c for c in celldyn_full.columns if 'alrt' in c.lower()]\n",
    "c_s_columns = [c for c in celldyn_full.columns if 'c_s_' in c.lower()]\n",
    "celldyn_full.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "celldyn_full = celldyn_full.assign(gender=celldyn_full.gender.map({'M':0, 'F':1}))\n",
    "celldyn_full.dropna(subset=['gender','draw_hour'], axis=0, inplace=True)\n",
    "celldyn_full.rename(columns={'studyid_alle_celldyn':'study_id', 'afname_dt': 'sample_dt'}, inplace=True)\n",
    "celldyn_full.set_index(['study_id', 'sample_dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celldyn_emb = pd.read_feather(\"L:\\laupodteam\\AIOS\\Bram\\data/CellDyn/artifacts/umap_euclidean_euclidean_spectral_dims_6_n_n_50_n_epochs_400_densmap_True_embedded_data.feather\")\n",
    "celldyn_emb = celldyn_emb.assign(sex=celldyn_emb.sex.map({'M':0, 'F':1}))\n",
    "celldyn_emb.dropna(subset=['sex','draw_hour'], axis=0, inplace=True)\n",
    "celldyn_emb.set_index(['study_id', 'sample_dt'], inplace=True)\n",
    "celldyn_emb = celldyn_emb.loc[celldyn_full.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_full = celldyn_full.reset_index()[['study_id', 'sample_dt']].drop_duplicates()\n",
    "in_emb = celldyn_emb.reset_index()[['study_id', 'sample_dt']].drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-collinearity removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11524/3731235490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     ('clst', clst)], \n\u001b[0;32m      7\u001b[0m                     verbose=True)\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mclst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 )\n\u001b[0;32m   1150\u001b[0m             ):\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\cluster\\_optics.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreachability_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredecessor_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_optics_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mmin_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                     )\n\u001b[0;32m    210\u001b[0m                 ):\n\u001b[1;32m--> 211\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\cluster\\_optics.py\u001b[0m in \u001b[0;36mcompute_optics_graph\u001b[1;34m(X, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;31m# the original OPTICS that only used epsilon range queries.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[1;31m# TODO: handle working_memory somehow?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m     core_distances_ = _compute_core_distances_(\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnbrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m     )\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\cluster\\_optics.py\u001b[0m in \u001b[0;36m_compute_core_distances_\u001b[1;34m(X, neighbors, min_samples, working_memory)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_n_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0mcore_distances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcore_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    820\u001b[0m         )\n\u001b[0;32m    821\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_pairwise_distances_reductions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m             results = ArgKmin.compute(\n\u001b[0m\u001b[0;32m    823\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m                 \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             return ArgKmin64.compute(\n\u001b[0m\u001b[0;32m    259\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx\u001b[0m in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreadpool_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_threadpool_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         modules = _ThreadpoolInfo(prefixes=self._prefixes,\n\u001b[0m\u001b[0;32m    269\u001b[0m                                   user_api=self._user_api)\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_if_incompatible_openmp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_load_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_modules_with_dyld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"win32\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_modules_with_enum_process_module_ex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_modules_with_dl_iterate_phdr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_find_modules_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m                 \u001b[1;31m# Store the module if it is supported and selected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_module_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mkernel_32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCloseHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_make_module_from_path\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefixes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0muser_api\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[0mmodule_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m                 \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[0;32m    604\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_api\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minternal_api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynlib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_RTLD_NOLOAD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_threads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_threads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_extra_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VENVS\\Envs\\hema_embedder\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36mget_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    644\u001b[0m                              lambda: None)\n\u001b[0;32m    645\u001b[0m         \u001b[0mget_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"OpenBLAS\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "tmp = celldyn_full[meas_columns].sample(1000).T\n",
    "stnd = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "clst = OPTICS(min_samples=5, leaf_size=10, metric=\"manhattan\", cluster_method=\"xi\", n_jobs=4)\n",
    "#clst = AffinityPropagation(damping=0.5)\n",
    "le_pipe = Pipeline([('stnd', stnd), \n",
    "                    ('clst', clst)], \n",
    "                    verbose=True)\n",
    "clst.fit(tmp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using cluster classifier-based model-selection\n",
    "\n",
    "i.e. what features are important in seperating patient clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster label\n",
    "\n",
    "# get statistic test of difference between clusters for each feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature expansion using  symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob metrics\n",
    "\n",
    "* We use the density profiles from HDBSCAN to characterize the blobbiness of the data\n",
    "* We use fuzzy c-means in combination with AUFPC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "sample_size = 25_000\n",
    "selection = in_full.sample(sample_size).reset_index(drop=True)\n",
    "\n",
    "kwargs = {\n",
    "    'min_cluster_size': 5, \n",
    "    'min_samples': 15, \n",
    "    'metric': 'manhattan',\n",
    "    'cluster_selection_method':'leaf'    \n",
    "}\n",
    "\n",
    "emb_index = celldyn_emb[celldyn_emb.index.isin(selection.itertuples(index=False))].index.drop_duplicates()\n",
    "full_index = celldyn_full[celldyn_full.index.isin(selection.itertuples(index=False))].index.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = celldyn_full.loc[full_index].reset_index().groupby(['study_id', 'sample_dt']).first()\n",
    "df_ = df[meas_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ranker = FunctionTransformer(lambda x: np.argsort(np.argsort(x, axis=0), axis=0))\n",
    "\n",
    "class PreTrainedTransformer:\n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler  \n",
    "        try:\n",
    "            self.labels_ = scaler.labels_\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        try:\n",
    "            return self.scaler.transform(X)\n",
    "        except:\n",
    "            return self.scaler.predict(X)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "def pipe_fitter_fixed(df, \n",
    "                scaler: Literal = ['standard', 'minmax', 'power'],\n",
    "                clusterer: Literal = ['kmeans', 'spectral', 'agglomerative'],\n",
    "                embedder: Literal = ['pca', 'umap', 'trimap', 'pacmap', 'RAW'],\n",
    "                pre_ranking: bool = True,\n",
    "                n_components: int = 6,\n",
    "                n_clusters: int = 10, \n",
    "                pipe=None,\n",
    "                pretrained=None,\n",
    "                cluster_init='k-means++'\n",
    "                ):\n",
    "    '''\n",
    "        in:\n",
    "            df: dataframe with measurements\n",
    "            clusterer: clusterer to use\n",
    "            embedder: embedder to use\n",
    "            pre_ranking: whether or not to use pre-ranking\n",
    "            n_components: number of components to use for embedding\n",
    "            n_clusters: number of clusters to use for clustering\n",
    "        returns fitted pipeline\n",
    "    '''\n",
    "    assert((isinstance(cluster_init, str)) | (isinstance(cluster_init, np.ndarray)) | (cluster_init is None)), \\\n",
    "    f'cluster_init should be either a string or a numpy array, is {type(cluster_init)}'\n",
    "    if isinstance(cluster_init, str):\n",
    "        assert(cluster_init in ['k-means++', 'random'])\n",
    "    if cluster_init is None:\n",
    "        cluster_init = 'k-means++'\n",
    "    \n",
    "    if scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler == 'power':\n",
    "        scaler = PowerTransformer()\n",
    "        \n",
    "    ######################################################    \n",
    "    \n",
    "    if (pipe is not None) & (pretrained=='clusterer'):\n",
    "        clusterer = PreTrainedTransformer(pipe.named_steps['clusterer'])\n",
    "    else:\n",
    "        if clusterer == 'spectral':\n",
    "            clusterer = SpectralClustering(n_clusters=n_clusters, random_state=123, n_init=10, affinity='rbf', n_neighbors=500)\n",
    "        elif clusterer == 'kmeans':\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=123, n_init=25, max_iter=1000, init=cluster_init)\n",
    "        elif clusterer == 'agglomerative':\n",
    "            clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='average', metric='manhattan')\n",
    "\n",
    "    ######################################################\n",
    "\n",
    "    if (pipe is not None) & (pretrained=='embedder'):\n",
    "        embedder = PreTrainedTransformer(pipe.named_steps['embedder'])\n",
    "    else:\n",
    "        if embedder == 'pca':\n",
    "            embedder = PCA(n_components=n_components, whiten=False)\n",
    "        elif embedder == 'umap':\n",
    "            embedder = umap.UMAP(n_components=n_components, n_neighbors=500, n_epochs=500,\n",
    "                                metric='manhattan', init='spectral',\n",
    "                                random_state=123, densmap=False, min_dist=0)\n",
    "        elif embedder == 'trimap':\n",
    "            pipeline_emb = clusterer.fit(\n",
    "                            trimap.TRIMAP(n_dims=n_components, n_inliers=12, n_outliers=4, \n",
    "                                        distance='manhattan', n_iters=500, lr=0.09).fit_transform(\n",
    "                                scaler.fit_transform(df)\n",
    "                                )\n",
    "                            )\n",
    "            return pipeline_emb\n",
    "        elif embedder == 'pacmap':\n",
    "            embedder = pacmap.PaCMAP(n_components=n_components, \n",
    "                                        n_neighbors=500, num_iters=500, lr=0.9, \n",
    "                                        apply_pca=False)\n",
    "        elif embedder == 'RAW':\n",
    "            embedder = None\n",
    "    \n",
    "    ######################################################\n",
    "    \n",
    "    if pre_ranking:\n",
    "        scaler = None\n",
    "        ranker = Ranker\n",
    "    \n",
    "    le_pipe_list = [('scaler', scaler)]\n",
    "    le_pipe_list += [('ranker', ranker)] if pre_ranking else []\n",
    "    le_pipe_list += [('embedder', embedder)]\n",
    "    le_pipe_list += [('clusterer', clusterer)]    \n",
    "    le_pipe = Pipeline(le_pipe_list)\n",
    "    le_pipe.fit(df)\n",
    "    \n",
    "    return le_pipe\n",
    "    \n",
    "def pipe_fitter_density(df, \n",
    "                scaler: Literal = ['standard', 'minmax', 'power'],\n",
    "                clusterer: Literal = ['hdbscan', 'optics'],\n",
    "                embedder: Literal = ['pca', 'umap', 'trimap', 'pacmap', 'RAW'],\n",
    "                pre_ranking: bool = True,\n",
    "                n_components = 6,\n",
    "                pipe=None,\n",
    "                pretrained=None):\n",
    "    '''\n",
    "        in:\n",
    "            df: dataframe with measurements\n",
    "            clusterer: clusterer to use\n",
    "            embedder: embedder to use\n",
    "            pre_ranking: whether or not to use pre-ranking\n",
    "            n_components: number of components to use for embedding\n",
    "            n_clusters: number of clusters to use for clustering\n",
    "        returns fitted pipeline\n",
    "    '''\n",
    "    \n",
    "    if scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler == 'power':\n",
    "        scaler = PowerTransformer()\n",
    "        \n",
    "    ######################################################    \n",
    "    \n",
    "    if (pipe is not None) & (pretrained=='clusterer'):\n",
    "        clusterer = PreTrainedTransformer(pipe.named_steps['clusterer'])\n",
    "    else:\n",
    "        if clusterer == 'optics':\n",
    "            clusterer = OPTICS(min_samples=15, metric='canberra', n_jobs=-1)\n",
    "        elif clusterer == 'hdbscan':\n",
    "            clusterer = HDBSCAN(min_cluster_size=5, min_samples=15, metric='canberra', cluster_selection_method='leaf')\n",
    "        else:\n",
    "            raise ValueError('Only optics and hdbscan are supported for density clustering')\n",
    "\n",
    "    ######################################################\n",
    "\n",
    "    if (pipe is not None) & (pretrained=='embedder'):\n",
    "        embedder = PreTrainedTransformer(pipe.named_steps['embedder'])\n",
    "    else:\n",
    "        if embedder == 'pca':\n",
    "            embedder = PCA(n_components=n_components, whiten=False)\n",
    "        elif embedder == 'umap':\n",
    "            embedder = umap.UMAP(n_components=n_components, n_neighbors=500, n_epochs=500,\n",
    "                                metric='manhattan', init='spectral',\n",
    "                                random_state=123, densmap=False, min_dist=0)\n",
    "        elif embedder == 'trimap':\n",
    "            pipeline_emb = clusterer.fit(\n",
    "                            trimap.TRIMAP(n_dims=n_components, n_inliers=12, n_outliers=4, \n",
    "                                        distance='manhattan', n_iters=500, lr=0.09).fit_transform(\n",
    "                                scaler.fit_transform(df)\n",
    "                                )\n",
    "                            )\n",
    "            return pipeline_emb\n",
    "        elif embedder == 'pacmap':\n",
    "            embedder = pacmap.PaCMAP(n_components=n_components, \n",
    "                                        n_neighbors=500, num_iters=500, lr=0.9, \n",
    "                                        apply_pca=False)\n",
    "        elif embedder == 'RAW':\n",
    "            embedder = None\n",
    "    \n",
    "    ######################################################\n",
    "    \n",
    "    if pre_ranking:\n",
    "        scaler = None\n",
    "        ranker = Ranker\n",
    "    \n",
    "    le_pipe_list = [('scaler', scaler)]\n",
    "    le_pipe_list += [('ranker', ranker)] if pre_ranking else []\n",
    "    le_pipe_list += [('embedder', embedder)]\n",
    "    le_pipe_list += [('clusterer', clusterer)]    \n",
    "    le_pipe = Pipeline(le_pipe_list)\n",
    "    le_pipe.fit(df)\n",
    "    \n",
    "    return le_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def enumerate_step(xs, start=0, step=1):\n",
    "    for x in xs:\n",
    "        yield (start, x)\n",
    "        start += step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot N components versus NMI/ARI with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ranking = False\n",
    "embedder = 'umap'\n",
    "clusterer = 'kmeans'\n",
    "scaler = 'standard'\n",
    "n_comp_range = range(2,32,1)\n",
    "n_clust_range = range(2,20,4)\n",
    "\n",
    "set_dict = {\n",
    "    'pre_ranking': pre_ranking,\n",
    "    'embedder': embedder,\n",
    "    'clusterer': clusterer,\n",
    "    'scaler': scaler    \n",
    "}\n",
    "\n",
    "res_dict = defaultdict(dict)\n",
    "raw_dict = {}\n",
    "pipe_res = None\n",
    "\n",
    "for n_clust in n_clust_range:\n",
    "    raw_dict[n_clust] = pipe_fitter_fixed(df_, n_components=None, n_clusters=n_clust,\n",
    "                                        pre_ranking=pre_ranking, embedder=None,\n",
    "                                        clusterer=clusterer, scaler=scaler, cluster_init=None)\n",
    "    \n",
    "for n_comp in tqdm(n_comp_range):    \n",
    "    cluster_centers = None\n",
    "    for n_clust in n_clust_range:        \n",
    "        pipe_res = pipe_fitter_fixed(df_, n_components=n_comp, n_clusters=n_clust,\n",
    "                            pre_ranking=pre_ranking, embedder=embedder, \n",
    "                            clusterer=clusterer, scaler=scaler, pipe=pipe_res, \n",
    "                            pretrained='embedder', cluster_init=cluster_centers)        \n",
    "        res_dict[(n_comp, n_clust)]['pipe'] = pipe_res\n",
    "        #cluster_centers = pipe_res.named_steps['clusterer'].cluster_centers_\n",
    "        #cluster_centers = np.vstack([cluster_centers, \n",
    "        #                            np.random.random((4,n_comp))])\n",
    "        \n",
    "    pipe_res = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HDBSCAN only (cluster.labels_>=0)\n",
    "for key, value in res_dict.items():\n",
    "    emb_labels = value['pipe']['clusterer'].labels_\n",
    "    full_labels = raw_dict[key[1]]['clusterer'].labels_\n",
    "    \n",
    "    res_dict[key]['ARI'] = ARI(emb_labels, \n",
    "                               full_labels)\n",
    "    res_dict[key]['NMI'] = NMI(emb_labels, \n",
    "                               full_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame([{'n_comp': k[0], \n",
    "                         'n_cluster': k[1], \n",
    "                         'ARI': v['ARI'], \n",
    "                         'NMI': v['NMI']} \n",
    "                        for k,v in res_dict.items()])\n",
    "plot_df = pd.melt(plot_df, id_vars=['n_comp', 'n_cluster'], value_vars=['ARI', 'NMI'], var_name='metric', value_name='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "sns.relplot(data=plot_df.query('~n_cluster.isin([]) & n_comp<33'), \n",
    "            x='n_comp', y='score', col='metric', hue='n_cluster', kind='line') #  palette=cm\n",
    "\n",
    "plt.ylim(0.21,1)\n",
    "plt.suptitle(f'{embedder} + {clusterer}. Sample size: {sample_size}.')\n",
    "plt.savefig(f'./../artifacts/{embedder}_{clusterer}_{sample_size}.eps', dpi=300, bbox_inches='tight')\n",
    "\n",
    "fname = \"__\".join([f\"{k}_{str(v)}\" for k,v in set_dict.items()]+[f'sample_size_{sample_size}'])\n",
    "plot_df.to_csv(f'./../artifacts/ClusterAlignment_{fname}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot N components versus number of retrieved HDBSCAN clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ranking = False\n",
    "embedder = 'pacmap'\n",
    "clusterer = 'hdbscan'\n",
    "scaler = 'standard'\n",
    "n_comp_range = range(2,64,1)\n",
    "\n",
    "set_dict = {\n",
    "    'pre_ranking': pre_ranking,\n",
    "    'embedder': embedder,\n",
    "    'clusterer': clusterer,\n",
    "    'scaler': scaler    \n",
    "}\n",
    "\n",
    "res_dict = defaultdict(dict)\n",
    "raw_dict = {}\n",
    "pipe_res = None\n",
    "\n",
    "\n",
    "for n_comp in tqdm(n_comp_range):        \n",
    "    pipe_res = pipe_fitter_density(df_, n_components=n_comp,\n",
    "                            pre_ranking=pre_ranking, embedder=embedder, \n",
    "                            clusterer=clusterer, scaler=scaler, pipe=None,\n",
    "                            pretrained='embedder')        \n",
    "    res_dict[n_comp]['pipe'] = pipe_res\n",
    "    pipe_res = None\n",
    "    \n",
    "raw_density = pipe_fitter_density(df_, n_components=n_comp,\n",
    "                                        pre_ranking=pre_ranking, embedder=None,\n",
    "                                        clusterer=clusterer, scaler=scaler)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outliers = np.bincount(raw_density.named_steps['clusterer'].labels_+1)[0]\n",
    "\n",
    "for comp, res in res_dict.items():\n",
    "    # scores like before using all clusters\n",
    "    if embedder=='trimap':\n",
    "        _res = res['pipe']\n",
    "    else:\n",
    "        _res = res['pipe']['clusterer']\n",
    "        \n",
    "    res_dict[comp]['ARI_full'] = ARI(_res.labels_, raw_density['clusterer'].labels_)\n",
    "    res_dict[comp]['NMI_full'] = NMI(_res.labels_, raw_density['clusterer'].labels_)\n",
    "    \n",
    "    # scores based on the non-outlier clusters only, as indicated by the raw_density results with -1\n",
    "    non_outlier_mask = raw_density['clusterer'].labels_>=0\n",
    "    res_dict[comp]['ARI_non_outlier'] = ARI(_res.labels_[non_outlier_mask], \n",
    "                                            raw_density['clusterer'].labels_[non_outlier_mask])\n",
    "    res_dict[comp]['NMI_non_outlier'] = NMI(_res.labels_[non_outlier_mask], \n",
    "                                            raw_density['clusterer'].labels_[non_outlier_mask])\n",
    "\n",
    "    # number of outliers\n",
    "    res_dict[comp]['num_outliers'] = sum(_res.labels_>=0)\n",
    "    \n",
    "    # number of clusters\n",
    "    res_dict[comp]['num_clusters'] = len(np.unique(_res.labels_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame([{'n_comp': k, \n",
    "                         'n_outliers': v['num_outliers'],\n",
    "                         'n_clusters': v['num_clusters'],\n",
    "                         'ARI_full': v['ARI_full'], \n",
    "                         'NMI_full': v['NMI_full'],\n",
    "                         'ARI_non_outlier': v['ARI_non_outlier'],\n",
    "                         'NMI_non_outlier': v['NMI_non_outlier']                         \n",
    "                         } \n",
    "                        for k,v in res_dict.items()])\n",
    "plot_df = pd.melt(plot_df, \n",
    "                  id_vars=['n_comp', 'n_clusters'],\n",
    "                  value_vars=['ARI_full', 'NMI_full', 'ARI_non_outlier', 'NMI_non_outlier', 'n_outliers'], \n",
    "                  var_name='metric', value_name='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=plot_df.query('~n_clusters.isin([]) & n_comp<64 & metric!=\"n_outliers\"'), \n",
    "            x='n_comp', y='score', hue='metric', palette='tab10') #  \n",
    "\n",
    "plt.suptitle(f'{embedder} + {clusterer}. Sample size: {sample_size}.')\n",
    "plt.savefig(f'./../artifacts/{embedder}_{clusterer}_{sample_size}.eps', dpi=300, bbox_inches='tight')\n",
    "\n",
    "fname = \"__\".join([f\"{k}_{str(v)}\" for k,v in set_dict.items()]+[f'sample_size_{sample_size}'])\n",
    "plot_df.to_csv(f'./../artifacts/ClusterHallucination_{fname}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_res = pd.read_csv('./../artifacts/ClusterHallucination_pre_ranking_False__embedder_pca__clusterer_hdbscan__scaler_standard__sample_size_25000.csv')\n",
    "umap_res = pd.read_csv('./../artifacts/ClusterHallucination_pre_ranking_False__embedder_umap__clusterer_hdbscan__scaler_standard__sample_size_25000.csv')\n",
    "trimap_res = pd.read_csv('./../artifacts/ClusterHallucination_pre_ranking_False__embedder_trimap__clusterer_hdbscan__scaler_standard__sample_size_25000.csv')\n",
    "pacmap_res = pd.read_csv('./../artifacts/ClusterHallucination_pre_ranking_False__embedder_pacmap__clusterer_hdbscan__scaler_standard__sample_size_25000.csv')\n",
    "plot_df = pd.concat([pca_res.assign(embedder='pca'), \n",
    "                     umap_res.assign(embedder='umap'),\n",
    "                     trimap_res.assign(embedder='trimap'),\n",
    "                     pacmap_res.assign(embedder='pacmap')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_num_clusters = np.unique(raw_density.named_steps['clusterer'].labels_).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_num_clusters = 11\n",
    "sns.lineplot(data=plot_df.query('~n_clusters.isin([]) & n_comp<64'), \n",
    "            x='n_comp', y='n_clusters', hue='embedder', palette='tab10')\n",
    "plt.axhline(raw_num_clusters, ls='--', color='k', label='raw')\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Number of clusters\")\n",
    "plt.savefig(f'./../artifacts/cluster_hallucination.eps', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xpca = pipeline_pca.named_steps['pca'].transform(df.loc[:, meas_columns])\n",
    "#Xemb = pipeline_emb.named_steps['embedder'].embedding_\n",
    "#Xorg = df.loc[:, meas_columns].values\n",
    "#perfClass = CDEmbeddingPerformance(n_neighbours=500, metric='manhattan', dcor_level=1)\n",
    "\n",
    "#rnd_sel = np.random.randint(0,100_000, 4000)\n",
    "#plt.plot(perfClass._return_dynamic_distance_correlation(Xpca[rnd_sel], Xorg[rnd_sel], n_bins=80))\n",
    "#plt.plot(perfClass._return_dynamic_distance_correlation(Xemb[rnd_sel], Xorg[rnd_sel], n_bins=80))\n",
    "#plt.legend(['pca', 'emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=2, figsize=(20, 12)) \n",
    "\n",
    "ordering = pipeline_emb.named_steps['clusterer'].ordering_\n",
    "reach = pipeline_emb.named_steps['clusterer'].reachability_[ordering]\n",
    "labels = pipeline_emb.named_steps['clusterer'].labels_[ordering]\n",
    "space = np.arange(labels.shape[0])\n",
    "num_categories = len(np.unique(labels))\n",
    "colormap = cm.get_cmap('viridis', num_categories)\n",
    "colors = [colormap(i) for i in range(num_categories)]\n",
    "\n",
    "for klass, color in zip(range(0, num_categories), colors):\n",
    "    Xk = space[labels == klass]\n",
    "    Rk = reach[labels == klass]\n",
    "    ax[0].plot(Xk, Rk, color, alpha=0.8)\n",
    "ax[0].plot(space[labels == -1], reach[labels == -1], \"k.\", alpha=0.3)\n",
    "#ax[0].plot(space, np.full_like(space, 2.0, dtype=float), \"k-\", alpha=0.5)\n",
    "#ax[0].plot(space, np.full_like(space, 0.5, dtype=float), \"k-.\", alpha=0.5)\n",
    "ax[0].set_ylabel(\"Reachability (epsilon distance)\")\n",
    "ax[0].set_xlim(1,15000)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "ordering = pipeline_full.named_steps['clusterer'].ordering_\n",
    "reach = pipeline_full.named_steps['clusterer'].reachability_[ordering]\n",
    "labels = pipeline_full.named_steps['clusterer'].labels_[ordering]\n",
    "space = np.arange(labels.shape[0])\n",
    "num_categories = len(np.unique(labels))\n",
    "colormap = cm.get_cmap('viridis', num_categories)\n",
    "colors = [colormap(i) for i in range(num_categories)]\n",
    "\n",
    "for klass, color in zip(range(0, num_categories), colors):\n",
    "    Xk = space[labels == klass]\n",
    "    Rk = reach[labels == klass]\n",
    "    ax[1].plot(Xk, Rk, color, alpha=0.3)\n",
    "ax[1].plot(space[labels == -1], reach[labels == -1], \"k.\", alpha=0.3)\n",
    "#ax[1].plot(space, np.full_like(space, 2.0, dtype=float), \"k-\", alpha=0.5)\n",
    "#ax[1].plot(space, np.full_like(space, 0.5, dtype=float), \"k-.\", alpha=0.5)\n",
    "ax[1].set_ylabel(\"Reachability (epsilon distance)\")\n",
    "ax[1].set_xlim(1,15000)\n",
    "ax[1].set_ylim(20,40)\n",
    "\n",
    "fig.suptitle(\"Reachability Plot\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapped Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "bclusters = clustering_bootstrapping(celldyn_full[meas_columns], n_clusters=10, n_bootstraps=60, sample_size=10000, clusterer='kmeans', n_init=1, align=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = celldyn_full.sample(10000, replace=True)[meas_columns]\n",
    "labels, centers, sizes = assign_clusters(test_set, bclusters, clusterer='kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import fowlkes_mallows_score, homogeneity_score, homogeneity_completeness_v_measure\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score, adjusted_mutual_info_score, pair_confusion_matrix\n",
    "\n",
    "cluster_overlap_list = []\n",
    "for i in range(labels.shape[0]):\n",
    "    for j in range(i+1, labels.shape[0]):\n",
    "        cluster_overlap_list.append(fowlkes_mallows_score(labels[i], labels[j]))\n",
    "\n",
    "plt.hist(cluster_overlap_list, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to re-order the cluster labels for each model such that the ordinal similarity between the models is maximized\n",
    "init_sim = np.sum(np.corrcoef(labels))/labels.shape[0]**2\n",
    "print(f\"Initial similarity: {init_sim}\")\n",
    "\n",
    "#  we want to align the cluster id's based on the cluster centers or the cluster assignmnents\n",
    "def align_cluster_assignments(labels: np.ndarray, centers: np.ndarray, method: str='centerwise', base_sel: int=0)-> np.ndarray:\n",
    "    '''\n",
    "        Centerwise, given, 1..N models, we align the 2,N models based on the cluster centers of model 1.\n",
    "        I.e. we find the cluster center in model 2 that is closest to the cluster center in model 1 and assign the cluster id's accordingly\n",
    "    '''\n",
    "    if method == 'centerwise':\n",
    "        base_centers = centers[base_sel]\n",
    "        # find the closest cluster center for each model\n",
    "        closest_centers = [[np.argmin([distance(bv.ravel(), _cv.ravel(), metric='cosine') \n",
    "                                            for _cv in cv]) \n",
    "                                                for bv in base_centers] \n",
    "                                                    for cv in centers[1:]\n",
    "                                                    ]\n",
    "        # now we have the closest cluster centers for each model, we can re-order the labels\n",
    "        aligned_labels = [np.array([closest_centers[i][l] for l in labels[base_sel]]) for i in range(len(closest_centers))]\n",
    "        aligned_centers = [centers[i][closest_centers[i]] for i in range(len(closest_centers))]\n",
    "\n",
    "    '''\n",
    "        TODO:\n",
    "    \tWe want to re-order the cluster labels for each model such that the ordinal similarity between the models is maximized.\n",
    "        We express this similarity with MCC (Matthews correlation coefficient)\n",
    "    '''\n",
    "    # first order the labels from the first model, use the order index\n",
    "    # to order the labels of the other models\n",
    "\n",
    "    # now change the labels of the other models to match the order of the first model\n",
    "\n",
    "\n",
    "    return np.vstack(aligned_labels), np.stack(aligned_centers), closest_centers\n",
    "\n",
    "aligned_labels, aligned_centers, closest_centers = align_cluster_assignments(labels, centers, method='centerwise', base_sel=0)\n",
    "print(f\"Aligned result: {np.sum(np.corrcoef(aligned_labels))/aligned_labels.shape[0]**2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment = pd.DataFrame(aligned_labels.T, columns=['cluster_assignment_'+str(i) for i in range(aligned_labels.shape[0])])\n",
    "cluster_assignment.index = test_set.index\n",
    "cluster_assignment = cluster_assignment.assign(cluster_entropy=cluster_assignment.apply(lambda x: entropy(np.histogram(x, bins=10, density=True)[0]), axis=1))\n",
    "cluster_assignment = cluster_assignment.assign(median_cluster=cluster_assignment.apply(lambda x: int(np.median(x)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = celldyn_emb.loc[cluster_assignment.index].join(cluster_assignment[['cluster_entropy', 'median_cluster']], how='inner')\n",
    "sns.scatterplot(data=plot_df, x='dim_2', y='dim_3', hue='median_cluster', palette='viridis', alpha=0.5, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 0\n",
    "for plot_count in range(100):\n",
    "    _v1, _v2 = random.sample(range(0, aligned_centers.shape[2]), 2)\n",
    "    if _v1 != _v2:\n",
    "        v1 = aligned_centers[:, cluster_num, _v1]\n",
    "        v2 = aligned_centers[:, cluster_num, _v2]\n",
    "        plt.scatter(v1, v2, c='r', alpha=0.05)\n",
    "plt.title(f\"Cluster-center spread for cluster {cluster_num}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMI and ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpcs = []\n",
    "\n",
    "df =celldyn_full[meas_columns] # ['age',  'gender']]\n",
    "#df = celldyn_emb[['dim_1', 'dim_2', 'dim_3', 'dim_4',  'dim_5', 'dim_6']]\n",
    "n_bootstrap = 30\n",
    "sample_count = 50000\n",
    "for n in tqdm(range(n_bootstrap)):\n",
    "    data = df.sample(sample_count)\n",
    "    _idx = data.index\n",
    "    \n",
    "    _fpcs = []\n",
    "    for ncenters, ax in enumerate(range(10),2):\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(data.T.values, c=ncenters, m=1.1, \n",
    "                                        error=0.005, maxiter=1000, init=None, seed=123)\n",
    "\n",
    "        # Store fpc values for later\n",
    "        _fpcs.append(fpc)\n",
    "    fpcs.append(_fpcs)\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    plt.plot(np.arange(2, 12), fpcs[i], alpha=0.1, c='b')\n",
    "\n",
    "plt.xlabel(\"Number of centers\")\n",
    "plt.ylabel(\"Fuzzy partition coefficient\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM evaluation\n",
    "\n",
    "* use ```model.aic``` and ```model.bic``` to evaluate GMM\n",
    "* use ```model.score_samples``` to evaluate BGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample data\n",
    "res_list = []\n",
    "n_bootstrap = 10\n",
    "sample_count = 5000\n",
    "n_clusters = 20\n",
    "\n",
    "df =celldyn_full[meas_columns] \n",
    "#df = celldyn_emb[['dim_1', 'dim_2', 'dim_3', 'dim_4',  'dim_5', 'dim_6', 'age', 'sex']]\n",
    "cluster_sizes = []\n",
    "for b in range(n_bootstrap):\n",
    "    data = df.sample(sample_count)\n",
    "    # extract the cluster centers and aic, bic using GMM for 1..10 clusters\n",
    "    \n",
    "    for n in range(2, n_clusters+1):\n",
    "        gmm = GMM(n_components=n, covariance_type='full', random_state=123, max_iter=1000).fit(data)\n",
    "        labels = gmm.predict(data)\n",
    "        cluster_sizes.append(np.bincount(labels))\n",
    "\n",
    "        #centers = gmm.means_\n",
    "        aic = gmm.aic(data)\n",
    "        bic = gmm.bic(data)\n",
    "        silhouette_avg = silhouette_score(data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(data.values, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(data.values, labels)\n",
    "        mean_log_likelihood = gmm.score(data)\n",
    "\n",
    "        # only keep the samples that are assigned to clusters with more than 50 samples\n",
    "        # this is to avoid the problem of having too few samples in a cluster\n",
    "        # and thus the cluster classification f1 score is not reliable\n",
    "\n",
    "        f1, f1_std = cluster_classification_f1(data.values, labels)\n",
    "        res_list.append({'bootstrap_round': b, \n",
    "                         'num_cluster': n, \n",
    "                         'AIC': aic, \n",
    "                         'BIC': bic, \n",
    "                         'silhouette_score': silhouette_avg,\n",
    "                         'davies_bouldin_score': davies_bouldin,\n",
    "                         'calinski_harabasz_score': calinski_harabasz,\n",
    "                         'score': mean_log_likelihood,\n",
    "                         'f1': f1,\n",
    "                         'f1_std': f1_std})\n",
    "res_df = pd.DataFrame(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(18, 25))\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='AIC', alpha=0.1, c='b',  ax=ax[0,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='BIC', alpha=0.1, c='b',  ax=ax[0,1])\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='silhouette_score', alpha=0.1, c='b',  ax=ax[1,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='davies_bouldin_score', alpha=0.1, c='b',  ax=ax[1,1])\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='calinski_harabasz_score', alpha=0.1, c='b',  ax=ax[2,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='score', alpha=0.1, c='b',  ax=ax[2,1])\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='f1', alpha=0.1, c='b',  ax=ax[3,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='f1_std', alpha=0.1, c='b',  ax=ax[3,1])\n",
    "\n",
    "fig.suptitle(\"GMM clustering, bootstrap N=10, sample size=5000\")\n",
    "ax[0,0].set_title(\"AIC\")\n",
    "ax[0,1].set_title(\"BIC\")\n",
    "\n",
    "ax[1,0].set_title(\"Silhouette score\")\n",
    "ax[1,1].set_title(\"David Bouldin score\")\n",
    "\n",
    "ax[2,0].set_title(\"Calinski Harabasz score\")\n",
    "ax[2,1].set_title(\"Mean log likelihood\")\n",
    "\n",
    "ax[3,0].set_title(\"F1 score\")\n",
    "ax[3,1].set_title(\"F1 score std\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample data\n",
    "res_list = []\n",
    "n_bootstrap = 20\n",
    "sample_count = 10000\n",
    "n_clusters = 20\n",
    "\n",
    "df =celldyn_full[meas_columns] \n",
    "#df = celldyn_emb[['dim_1', 'dim_2', 'dim_3', 'dim_4',  'dim_5', 'dim_6', 'age', 'sex']]\n",
    "for b in tqdm(range(n_bootstrap)):\n",
    "    data = df.sample(sample_count)\n",
    "    # extract the cluster centers and aic, bic using GMM for 1..10 clusters    \n",
    "    \n",
    "    gmm = BGMM(n_components=n_clusters, covariance_type='full', random_state=123, max_iter=1000).fit(data)\n",
    "    #labels = gmm.predict(data)\n",
    "    #centers = gmm.means_\n",
    "    res_list.append({'bootstrap_round': b, \n",
    "                     'mean_log_likelihood': gmm.score(data)})\n",
    "res_df = pd.DataFrame(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =celldyn_full[meas_columns] \n",
    "data = df.sample(1000)\n",
    "n_comp = 3\n",
    "gmm = BGMM(n_components=n_comp, covariance_type='full', random_state=123, max_iter=1000).fit(data)\n",
    "bgmm_res = gmm.predict_proba(data)\n",
    "AIC = gmm.aic(data)\n",
    "BIC = gmm.bic(data)\n",
    "data['log_likelihood_gmm'] = gmm.score_samples(data)\n",
    "data[[f'cluster_{i}' for i in range(n_comp)]] = bgmm_res\n",
    "res_df = pd.DataFrame(data=bgmm_res, columns=[f'cluster_{i}' for i in range(n_comp)])\n",
    "res_df.index=data.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[f'cluster_{i}' for i in range(n_comp)]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIC, BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = 'cluster_2'\n",
    "plot_df = celldyn_emb.loc[data.index].join(data[['log_likelihood_gmm', clust]], how='inner')\n",
    "sns.scatterplot(data=plot_df[plot_df.log_likelihood_gmm>=0], x='dim_1', y='dim_3', hue=clust, palette='viridis', alpha=0.5, legend=True)\n",
    "sns.scatterplot(data=plot_df[plot_df.log_likelihood_gmm<0], x='dim_1', y='dim_3', color='black', alpha=0.1, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da1a1f9fee48aeb25414ae45b99f00113784206b411b315deabc951a0f32f6bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "#try:\n",
    "#    os.chdir(\"L:\\laupodteam\\AIOS\\Huibert-Jan\\Celldynclustering\\celldyn_embedder\")\n",
    "#except FileNotFoundError:\n",
    "#    os.chdir('C:/Users/Huibert-Jan/Documents/Werk/UMCU/celldyn_embedder')    \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import pacmap\n",
    "import umap\n",
    "#import trimap\n",
    "\n",
    "#from scipy.stats import chisquare, chi2_contingency, pearsonr\n",
    "#from scipy.stats import kendalltau,spearmanr, weightedtau, theilslopes, wilcoxon, ttest_rel\n",
    "#from scipy.spatial import distance\n",
    "#import dcor\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.manifold import Isomap, MDS, SpectralEmbedding\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE, TSNE, smacof, trustworthiness\n",
    "from sklearn.cluster import KMeans, BisectingKMeans, OPTICS, affinity_propagation, AgglomerativeClustering\n",
    "# add GMM\n",
    "from sklearn.mixture import GaussianMixture as GMM, BayesianGaussianMixture as BGMM\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "\n",
    "#from sklearn.metrics import rand_score, adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "#from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "from hembedder.utils.distance import poincarre_dist, hyperboloid_dist, fractional_distance, Distance\n",
    "#from hembedder.utils.quality_metrics import CDEmbeddingPerformance\n",
    "#import numpy.linalg as la\n",
    "#import torch \n",
    "\n",
    "#from numba import njit\n",
    "\n",
    "#import faiss\n",
    "import gc\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pysr import PySRRegressor\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "from sklego.meta import ZeroInflatedRegressor\n",
    "#from lineartree import LinearTreeRegressor, LinearForestRegressor\n",
    "\n",
    "from xgboost import XGBRegressor, XGBRFClassifier\n",
    "\n",
    "from typing import List, Tuple, Iterable, Callable\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take class BGMM and extend with a function called bic\n",
    "class BGMM(BGMM):\n",
    "    def _n_parameters(self, X):\n",
    "          \"\"\"Return the number of free parameters in the model.\"\"\"\n",
    "          _, n_features = self.means_.shape\n",
    "          # Number of effective components equals the number of unique labels\n",
    "          n_effect_comp = len(np.unique(self.predict(X)))\n",
    "          #n_effect_comp = self.n_components\n",
    "          if self.covariance_type == 'full':\n",
    "              cov_params = n_effect_comp * n_features * (n_features + 1) / 2.\n",
    "          elif self.covariance_type == 'diag':\n",
    "              cov_params = n_effect_comp * n_features\n",
    "          elif self.covariance_type == 'tied':\n",
    "              cov_params = n_features * (n_features + 1) / 2.\n",
    "          elif self.covariance_type == 'spherical':\n",
    "              cov_params = n_effect_comp\n",
    "          mean_params = n_features * n_effect_comp\n",
    "          return int(cov_params + mean_params + n_effect_comp - 1)\n",
    "\n",
    "    def bic(self, X):\n",
    "        \"\"\"Bayesian information criterion for the current model on the input X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape (n_samples, n_dimensions)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bic : float\n",
    "            The lower the better.\n",
    "        \"\"\"\n",
    "        return (-2 * self.score(X) * X.shape[0] + self._n_parameters(X) * np.log(X.shape[0]))\n",
    "\n",
    "    def aic(self, X):\n",
    "        \"\"\"Akaike information criterion for the current model on the input X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape (n_samples, n_dimensions)\n",
    "+\n",
    "        Returns\n",
    "        -------\n",
    "        aic : float\n",
    "            The lower the better.\n",
    "        \"\"\"\n",
    "        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_classification_f1(X, labels):\n",
    "    '''\n",
    "        create a logistic regression classifier to predict cluster labels\n",
    "        , determine the stratified cross-validated F1 score of the model.\n",
    "    '''\n",
    "    # create a logistic regression classifier to predict cluster labels\n",
    "    clf = LogisticRegression(random_state=0, penalty='elasticnet', solver='saga', l1_ratio=0.1,\n",
    "                                multi_class='multinomial', max_iter=5000)\n",
    "\n",
    "    # determine the stratified cross-validated F1 score of the model.\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in skf.split(X, labels):\n",
    "        clf.fit(X[train_index], labels[train_index])\n",
    "        f1_scores.append(f1_score(labels[test_index], clf.predict(X[test_index]), average='weighted'))\n",
    "\n",
    "    return np.mean(f1_scores), np.std(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to run a clustering algorithm in a bootstrapping fashion\n",
    "# the outcome is a list of clustering models\n",
    "def clustering_bootstrapping(data: pd.DataFrame, \n",
    "                             n_clusters: int=10,\n",
    "                             n_bootstraps: int=100, \n",
    "                             sample_size: int=50000,\n",
    "                             clusterer: str='kmeans',\n",
    "                             align: bool=False,\n",
    "                             **kwargs)-> List[Callable]:\n",
    "    if kwargs.get('n_init', None) is None:\n",
    "        n_init = 10\n",
    "    else:\n",
    "        n_init = kwargs.get('n_init')\n",
    "    \n",
    "    model_list = []\n",
    "    for i in tqdm(range(n_bootstraps)):\n",
    "        # sample data\n",
    "        sample = data.sample(sample_size, replace=True)\n",
    "        # cluster sample\n",
    "        if clusterer == 'kmeans':\n",
    "            if (i==0) | (align==False):\n",
    "                model = KMeans(n_clusters=n_clusters, random_state=123, n_init=n_init)\n",
    "                model.fit(sample)\n",
    "                init_means = model.cluster_centers_\n",
    "            else:\n",
    "                model = KMeans(n_clusters=n_clusters, random_state=123, init=init_means, n_init=n_init)\n",
    "                model.fit(sample)\n",
    "        elif clusterer == 'optics':\n",
    "            model = OPTICS(min_samples=100, n_jobs=-1)\n",
    "            model.fit(sample)\n",
    "        elif clusterer == 'hierarchical':\n",
    "            model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "            model.fit(sample)\n",
    "        elif clusterer == 'gmm':\n",
    "            if (i==0) | (align==False):\n",
    "                model = GMM(n_components=n_clusters, random_state=123)\n",
    "                model.fit(sample)\n",
    "                init_means = model.means_\n",
    "            else:\n",
    "                model = GMM(n_components=n_clusters, random_state=123, means_init=init_means)\n",
    "                model.fit(sample)\n",
    "        elif clusterer == 'bgmm':\n",
    "            if (i==0) | (align==False):\n",
    "                model = BGMM(n_components=n_clusters, random_state=123, max_iter=500)\n",
    "                model.fit(sample)\n",
    "                init_means = model.mean_prior_\n",
    "            else:\n",
    "                model = BGMM(n_components=n_clusters, random_state=123, mean_prior=init_means, max_iter=500)\n",
    "                model.fit(sample)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown clustering algorithm\")\n",
    "        model_list.append(model)\n",
    "    return model_list\n",
    "    \n",
    "def assign_clusters(data: pd.DataFrame, \n",
    "                    model_list: List[Callable], \n",
    "                    clusterer='kmeans') -> pd.DataFrame:\n",
    "    \n",
    "    # get cluster labels\n",
    "    if clusterer in ['kmeans']:\n",
    "        labels = np.array([model.predict(data) for model in model_list])\n",
    "        # get cluster centers\n",
    "        centers = np.array([model.cluster_centers_ for model in model_list])\n",
    "        # get cluster sizes\n",
    "        sizes = np.array([np.bincount(label, minlength=centers.shape[1]) for label in labels])\n",
    "    elif clusterer in ['gmm', 'bgmm']:\n",
    "        labels = np.array([model.predict(data) for model in model_list])\n",
    "        # get cluster centers\n",
    "        centers = np.array([model.means_ for model in model_list])\n",
    "        # get cluster sizes\n",
    "        sizes = np.array([np.bincount(label, minlength=centers.shape[1]) for label in labels])        \n",
    "    return labels, centers, sizes \n",
    "\n",
    "\n",
    "def distance(v1, v2, metric='euclidean'):\n",
    "    ''' Calculate the distance between two vectors '''\n",
    "    if metric=='euclidean':\n",
    "        return np.linalg.norm(v1 - v2)\n",
    "    elif metric=='cosine':\n",
    "        return 1 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    elif metric=='poincarre':\n",
    "        return poincarre_dist(v1, v2)\n",
    "    elif metric=='manhattan':\n",
    "        return np.linalg.norm(v1 - v2, ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "celldyn_full = pd.read_feather(\"L:/laupodteam/AIOS/Bram/data/CellDyn/artifacts/celldyn_FULL_transformed_df_updated.feather\")\n",
    "\n",
    "meas_columns = [c for c in celldyn_full.columns if ('c_b' in c) | (\"COMBO\" in c)]\n",
    "mode_columns = [c for c in celldyn_full.columns if 'c_m' in c]\n",
    "alrt_columns = [c for c in celldyn_full.columns if 'alrt' in c.lower()]\n",
    "c_s_columns = [c for c in celldyn_full.columns if 'c_s_' in c.lower()]\n",
    "celldyn_full.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "celldyn_full = celldyn_full.assign(gender=celldyn_full.gender.map({'M':0, 'F':1}))\n",
    "celldyn_full.dropna(subset=['gender','draw_hour'], axis=0, inplace=True)\n",
    "celldyn_full.rename(columns={'studyid_alle_celldyn':'study_id', 'afname_dt': 'sample_dt'}, inplace=True)\n",
    "celldyn_full.set_index(['study_id', 'sample_dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "celldyn_emb = pd.read_feather(\"L:\\laupodteam\\AIOS\\Bram\\data/CellDyn/artifacts/umap_euclidean_euclidean_spectral_dims_6_n_n_50_n_epochs_400_densmap_True_embedded_data.feather\")\n",
    "celldyn_emb = celldyn_emb.assign(sex=celldyn_emb.sex.map({'M':0, 'F':1}))\n",
    "celldyn_emb.dropna(subset=['sex','draw_hour'], axis=0, inplace=True)\n",
    "celldyn_emb.set_index(['study_id', 'sample_dt'], inplace=True)\n",
    "celldyn_emb = celldyn_emb.loc[celldyn_full.index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using sparse coding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature expansion using  symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob metrics\n",
    "\n",
    "* We use the density profiles from HDBSCAN to characterize the blobbiness of the data\n",
    "* We use fuzzy c-means in combination with AUFPC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>analysis_dt</th>\n",
       "      <th>draw_hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_id</th>\n",
       "      <th>sample_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5.357638e+09</th>\n",
       "      <th>2005-01-06 14:33:00</th>\n",
       "      <td>4.943448</td>\n",
       "      <td>5.360318</td>\n",
       "      <td>7.086178</td>\n",
       "      <td>11.428235</td>\n",
       "      <td>5.570265</td>\n",
       "      <td>6.434824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>2005-01-06 15:15:50</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4.495800e+09</th>\n",
       "      <th>2005-01-06 14:05:00</th>\n",
       "      <td>5.612035</td>\n",
       "      <td>6.217134</td>\n",
       "      <td>5.532863</td>\n",
       "      <td>9.442817</td>\n",
       "      <td>4.809890</td>\n",
       "      <td>5.284393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>2005-01-06 15:16:27</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06 14:05:00</th>\n",
       "      <td>5.149362</td>\n",
       "      <td>6.366641</td>\n",
       "      <td>6.327810</td>\n",
       "      <td>10.439454</td>\n",
       "      <td>4.691250</td>\n",
       "      <td>6.643110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>2005-01-06 15:45:11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5.757731e+09</th>\n",
       "      <th>2005-01-06 13:58:00</th>\n",
       "      <td>4.018103</td>\n",
       "      <td>6.107883</td>\n",
       "      <td>8.214622</td>\n",
       "      <td>11.132696</td>\n",
       "      <td>5.023906</td>\n",
       "      <td>6.875960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41</td>\n",
       "      <td>2005-01-06 15:46:23</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06 13:58:00</th>\n",
       "      <td>4.032622</td>\n",
       "      <td>5.465497</td>\n",
       "      <td>7.555088</td>\n",
       "      <td>10.690550</td>\n",
       "      <td>4.768372</td>\n",
       "      <td>7.177920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41</td>\n",
       "      <td>2005-01-06 15:39:02</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.247108e+09</th>\n",
       "      <th>2015-12-16 08:00:00</th>\n",
       "      <td>3.135144</td>\n",
       "      <td>7.302594</td>\n",
       "      <td>7.445310</td>\n",
       "      <td>7.224091</td>\n",
       "      <td>7.257387</td>\n",
       "      <td>6.890238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51</td>\n",
       "      <td>2015-12-16 08:47:44</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3.965944e+09</th>\n",
       "      <th>2015-12-16 08:00:00</th>\n",
       "      <td>2.594149</td>\n",
       "      <td>7.207503</td>\n",
       "      <td>5.422307</td>\n",
       "      <td>7.959364</td>\n",
       "      <td>5.165495</td>\n",
       "      <td>5.265145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59</td>\n",
       "      <td>2015-12-16 11:59:35</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16 08:00:00</th>\n",
       "      <td>2.551686</td>\n",
       "      <td>7.209139</td>\n",
       "      <td>5.414259</td>\n",
       "      <td>7.901662</td>\n",
       "      <td>5.160422</td>\n",
       "      <td>5.219317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59</td>\n",
       "      <td>2015-12-16 09:28:53</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5.078079e+09</th>\n",
       "      <th>2015-12-16 10:42:00</th>\n",
       "      <td>4.681038</td>\n",
       "      <td>6.951552</td>\n",
       "      <td>7.408444</td>\n",
       "      <td>9.665081</td>\n",
       "      <td>6.937300</td>\n",
       "      <td>7.691802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34</td>\n",
       "      <td>2015-12-16 11:14:26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16 10:42:00</th>\n",
       "      <td>4.611025</td>\n",
       "      <td>6.910679</td>\n",
       "      <td>7.333861</td>\n",
       "      <td>9.287843</td>\n",
       "      <td>6.926658</td>\n",
       "      <td>7.895717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34</td>\n",
       "      <td>2015-12-16 12:32:10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3846746 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     dim_1     dim_2     dim_3      dim_4  \\\n",
       "study_id     sample_dt                                                      \n",
       "5.357638e+09 2005-01-06 14:33:00  4.943448  5.360318  7.086178  11.428235   \n",
       "4.495800e+09 2005-01-06 14:05:00  5.612035  6.217134  5.532863   9.442817   \n",
       "             2005-01-06 14:05:00  5.149362  6.366641  6.327810  10.439454   \n",
       "5.757731e+09 2005-01-06 13:58:00  4.018103  6.107883  8.214622  11.132696   \n",
       "             2005-01-06 13:58:00  4.032622  5.465497  7.555088  10.690550   \n",
       "...                                    ...       ...       ...        ...   \n",
       "3.247108e+09 2015-12-16 08:00:00  3.135144  7.302594  7.445310   7.224091   \n",
       "3.965944e+09 2015-12-16 08:00:00  2.594149  7.207503  5.422307   7.959364   \n",
       "             2015-12-16 08:00:00  2.551686  7.209139  5.414259   7.901662   \n",
       "5.078079e+09 2015-12-16 10:42:00  4.681038  6.951552  7.408444   9.665081   \n",
       "             2015-12-16 10:42:00  4.611025  6.910679  7.333861   9.287843   \n",
       "\n",
       "                                     dim_5     dim_6  sex  age  \\\n",
       "study_id     sample_dt                                           \n",
       "5.357638e+09 2005-01-06 14:33:00  5.570265  6.434824  0.0   53   \n",
       "4.495800e+09 2005-01-06 14:05:00  4.809890  5.284393  0.0   55   \n",
       "             2005-01-06 14:05:00  4.691250  6.643110  0.0   55   \n",
       "5.757731e+09 2005-01-06 13:58:00  5.023906  6.875960  0.0   41   \n",
       "             2005-01-06 13:58:00  4.768372  7.177920  0.0   41   \n",
       "...                                    ...       ...  ...  ...   \n",
       "3.247108e+09 2015-12-16 08:00:00  7.257387  6.890238  0.0   51   \n",
       "3.965944e+09 2015-12-16 08:00:00  5.165495  5.265145  1.0   59   \n",
       "             2015-12-16 08:00:00  5.160422  5.219317  1.0   59   \n",
       "5.078079e+09 2015-12-16 10:42:00  6.937300  7.691802  1.0   34   \n",
       "             2015-12-16 10:42:00  6.926658  7.895717  1.0   34   \n",
       "\n",
       "                                         analysis_dt  draw_hour  \n",
       "study_id     sample_dt                                           \n",
       "5.357638e+09 2005-01-06 14:33:00 2005-01-06 15:15:50         14  \n",
       "4.495800e+09 2005-01-06 14:05:00 2005-01-06 15:16:27         14  \n",
       "             2005-01-06 14:05:00 2005-01-06 15:45:11         14  \n",
       "5.757731e+09 2005-01-06 13:58:00 2005-01-06 15:46:23         13  \n",
       "             2005-01-06 13:58:00 2005-01-06 15:39:02         13  \n",
       "...                                              ...        ...  \n",
       "3.247108e+09 2015-12-16 08:00:00 2015-12-16 08:47:44          8  \n",
       "3.965944e+09 2015-12-16 08:00:00 2015-12-16 11:59:35          8  \n",
       "             2015-12-16 08:00:00 2015-12-16 09:28:53          8  \n",
       "5.078079e+09 2015-12-16 10:42:00 2015-12-16 11:14:26         10  \n",
       "             2015-12-16 10:42:00 2015-12-16 12:32:10         10  \n",
       "\n",
       "[3846746 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer = HDBSCAN(min_cluster_size=100, min_samples=100, metric='euclidean', cluster_selection_method='leaf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapped Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "bclusters = clustering_bootstrapping(celldyn_full[meas_columns], n_clusters=10, n_bootstraps=60, sample_size=10000, clusterer='kmeans', n_init=1, align=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = celldyn_full.sample(10000, replace=True)[meas_columns]\n",
    "labels, centers, sizes = assign_clusters(test_set, bclusters, clusterer='kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import fowlkes_mallows_score, homogeneity_score, homogeneity_completeness_v_measure\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score, adjusted_mutual_info_score, pair_confusion_matrix\n",
    "\n",
    "cluster_overlap_list = []\n",
    "for i in range(labels.shape[0]):\n",
    "    for j in range(i+1, labels.shape[0]):\n",
    "        cluster_overlap_list.append(fowlkes_mallows_score(labels[i], labels[j]))\n",
    "\n",
    "plt.hist(cluster_overlap_list, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to re-order the cluster labels for each model such that the ordinal similarity between the models is maximized\n",
    "init_sim = np.sum(np.corrcoef(labels))/labels.shape[0]**2\n",
    "print(f\"Initial similarity: {init_sim}\")\n",
    "\n",
    "#  we want to align the cluster id's based on the cluster centers or the cluster assignmnents\n",
    "def align_cluster_assignments(labels: np.ndarray, centers: np.ndarray, method: str='centerwise', base_sel: int=0)-> np.ndarray:\n",
    "    '''\n",
    "        Centerwise, given, 1..N models, we align the 2,N models based on the cluster centers of model 1.\n",
    "        I.e. we find the cluster center in model 2 that is closest to the cluster center in model 1 and assign the cluster id's accordingly\n",
    "    '''\n",
    "    if method == 'centerwise':\n",
    "        base_centers = centers[base_sel]\n",
    "        # find the closest cluster center for each model\n",
    "        closest_centers = [[np.argmin([distance(bv.ravel(), _cv.ravel(), metric='cosine') \n",
    "                                            for _cv in cv]) \n",
    "                                                for bv in base_centers] \n",
    "                                                    for cv in centers[1:]\n",
    "                                                    ]\n",
    "        # now we have the closest cluster centers for each model, we can re-order the labels\n",
    "        aligned_labels = [np.array([closest_centers[i][l] for l in labels[base_sel]]) for i in range(len(closest_centers))]\n",
    "        aligned_centers = [centers[i][closest_centers[i]] for i in range(len(closest_centers))]\n",
    "\n",
    "    '''\n",
    "        TODO:\n",
    "    \tWe want to re-order the cluster labels for each model such that the ordinal similarity between the models is maximized.\n",
    "        We express this similarity with MCC (Matthews correlation coefficient)\n",
    "    '''\n",
    "    # first order the labels from the first model, use the order index\n",
    "    # to order the labels of the other models\n",
    "\n",
    "    # now change the labels of the other models to match the order of the first model\n",
    "\n",
    "\n",
    "    return np.vstack(aligned_labels), np.stack(aligned_centers), closest_centers\n",
    "\n",
    "aligned_labels, aligned_centers, closest_centers = align_cluster_assignments(labels, centers, method='centerwise', base_sel=0)\n",
    "print(f\"Aligned result: {np.sum(np.corrcoef(aligned_labels))/aligned_labels.shape[0]**2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment = pd.DataFrame(aligned_labels.T, columns=['cluster_assignment_'+str(i) for i in range(aligned_labels.shape[0])])\n",
    "cluster_assignment.index = test_set.index\n",
    "cluster_assignment = cluster_assignment.assign(cluster_entropy=cluster_assignment.apply(lambda x: entropy(np.histogram(x, bins=10, density=True)[0]), axis=1))\n",
    "cluster_assignment = cluster_assignment.assign(median_cluster=cluster_assignment.apply(lambda x: int(np.median(x)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = celldyn_emb.loc[cluster_assignment.index].join(cluster_assignment[['cluster_entropy', 'median_cluster']], how='inner')\n",
    "sns.scatterplot(data=plot_df, x='dim_2', y='dim_3', hue='median_cluster', palette='viridis', alpha=0.5, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 0\n",
    "for plot_count in range(100):\n",
    "    _v1, _v2 = random.sample(range(0, aligned_centers.shape[2]), 2)\n",
    "    if _v1 != _v2:\n",
    "        v1 = aligned_centers[:, cluster_num, _v1]\n",
    "        v2 = aligned_centers[:, cluster_num, _v2]\n",
    "        plt.scatter(v1, v2, c='r', alpha=0.05)\n",
    "plt.title(f\"Cluster-center spread for cluster {cluster_num}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMI and ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpcs = []\n",
    "\n",
    "df =celldyn_full[meas_columns] # ['age',  'gender']]\n",
    "#df = celldyn_emb[['dim_1', 'dim_2', 'dim_3', 'dim_4',  'dim_5', 'dim_6']]\n",
    "n_bootstrap = 30\n",
    "sample_count = 50000\n",
    "for n in tqdm(range(n_bootstrap)):\n",
    "    data = df.sample(sample_count)\n",
    "    _idx = data.index\n",
    "    \n",
    "    _fpcs = []\n",
    "    for ncenters, ax in enumerate(range(10),2):\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(data.T.values, c=ncenters, m=1.1, \n",
    "                                        error=0.005, maxiter=1000, init=None, seed=123)\n",
    "\n",
    "        # Store fpc values for later\n",
    "        _fpcs.append(fpc)\n",
    "    fpcs.append(_fpcs)\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    plt.plot(np.arange(2, 12), fpcs[i], alpha=0.1, c='b')\n",
    "\n",
    "plt.xlabel(\"Number of centers\")\n",
    "plt.ylabel(\"Fuzzy partition coefficient\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM evaluation\n",
    "\n",
    "* use ```model.aic``` and ```model.bic``` to evaluate GMM\n",
    "* use ```model.score_samples``` to evaluate BGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample data\n",
    "res_list = []\n",
    "n_bootstrap = 10\n",
    "sample_count = 5000\n",
    "n_clusters = 20\n",
    "\n",
    "df =celldyn_full[meas_columns] \n",
    "#df = celldyn_emb[['dim_1', 'dim_2', 'dim_3', 'dim_4',  'dim_5', 'dim_6', 'age', 'sex']]\n",
    "cluster_sizes = []\n",
    "for b in range(n_bootstrap):\n",
    "    data = df.sample(sample_count)\n",
    "    # extract the cluster centers and aic, bic using GMM for 1..10 clusters\n",
    "    \n",
    "    for n in range(2, n_clusters+1):\n",
    "        gmm = GMM(n_components=n, covariance_type='full', random_state=123, max_iter=1000).fit(data)\n",
    "        labels = gmm.predict(data)\n",
    "        cluster_sizes.append(np.bincount(labels))\n",
    "\n",
    "        #centers = gmm.means_\n",
    "        aic = gmm.aic(data)\n",
    "        bic = gmm.bic(data)\n",
    "        silhouette_avg = silhouette_score(data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(data.values, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(data.values, labels)\n",
    "        mean_log_likelihood = gmm.score(data)\n",
    "\n",
    "        # only keep the samples that are assigned to clusters with more than 50 samples\n",
    "        # this is to avoid the problem of having too few samples in a cluster\n",
    "        # and thus the cluster classification f1 score is not reliable\n",
    "\n",
    "        f1, f1_std = cluster_classification_f1(data.values, labels)\n",
    "        res_list.append({'bootstrap_round': b, \n",
    "                         'num_cluster': n, \n",
    "                         'AIC': aic, \n",
    "                         'BIC': bic, \n",
    "                         'silhouette_score': silhouette_avg,\n",
    "                         'davies_bouldin_score': davies_bouldin,\n",
    "                         'calinski_harabasz_score': calinski_harabasz,\n",
    "                         'score': mean_log_likelihood,\n",
    "                         'f1': f1,\n",
    "                         'f1_std': f1_std})\n",
    "res_df = pd.DataFrame(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(18, 25))\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='AIC', alpha=0.1, c='b',  ax=ax[0,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='BIC', alpha=0.1, c='b',  ax=ax[0,1])\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='silhouette_score', alpha=0.1, c='b',  ax=ax[1,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='davies_bouldin_score', alpha=0.1, c='b',  ax=ax[1,1])\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='calinski_harabasz_score', alpha=0.1, c='b',  ax=ax[2,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='score', alpha=0.1, c='b',  ax=ax[2,1])\n",
    "\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='f1', alpha=0.1, c='b',  ax=ax[3,0])\n",
    "sns.lineplot(data=res_df, x='num_cluster', y='f1_std', alpha=0.1, c='b',  ax=ax[3,1])\n",
    "\n",
    "fig.suptitle(\"GMM clustering, bootstrap N=10, sample size=5000\")\n",
    "ax[0,0].set_title(\"AIC\")\n",
    "ax[0,1].set_title(\"BIC\")\n",
    "\n",
    "ax[1,0].set_title(\"Silhouette score\")\n",
    "ax[1,1].set_title(\"David Bouldin score\")\n",
    "\n",
    "ax[2,0].set_title(\"Calinski Harabasz score\")\n",
    "ax[2,1].set_title(\"Mean log likelihood\")\n",
    "\n",
    "ax[3,0].set_title(\"F1 score\")\n",
    "ax[3,1].set_title(\"F1 score std\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample data\n",
    "res_list = []\n",
    "n_bootstrap = 20\n",
    "sample_count = 10000\n",
    "n_clusters = 20\n",
    "\n",
    "df =celldyn_full[meas_columns] \n",
    "#df = celldyn_emb[['dim_1', 'dim_2', 'dim_3', 'dim_4',  'dim_5', 'dim_6', 'age', 'sex']]\n",
    "for b in tqdm(range(n_bootstrap)):\n",
    "    data = df.sample(sample_count)\n",
    "    # extract the cluster centers and aic, bic using GMM for 1..10 clusters    \n",
    "    \n",
    "    gmm = BGMM(n_components=n_clusters, covariance_type='full', random_state=123, max_iter=1000).fit(data)\n",
    "    #labels = gmm.predict(data)\n",
    "    #centers = gmm.means_\n",
    "    res_list.append({'bootstrap_round': b, \n",
    "                     'mean_log_likelihood': gmm.score(data)})\n",
    "res_df = pd.DataFrame(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =celldyn_full[meas_columns] \n",
    "data = df.sample(1000)\n",
    "n_comp = 3\n",
    "gmm = BGMM(n_components=n_comp, covariance_type='full', random_state=123, max_iter=1000).fit(data)\n",
    "bgmm_res = gmm.predict_proba(data)\n",
    "AIC = gmm.aic(data)\n",
    "BIC = gmm.bic(data)\n",
    "data['log_likelihood_gmm'] = gmm.score_samples(data)\n",
    "data[[f'cluster_{i}' for i in range(n_comp)]] = bgmm_res\n",
    "res_df = pd.DataFrame(data=bgmm_res, columns=[f'cluster_{i}' for i in range(n_comp)])\n",
    "res_df.index=data.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[f'cluster_{i}' for i in range(n_comp)]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIC, BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = 'cluster_2'\n",
    "plot_df = celldyn_emb.loc[data.index].join(data[['log_likelihood_gmm', clust]], how='inner')\n",
    "sns.scatterplot(data=plot_df[plot_df.log_likelihood_gmm>=0], x='dim_1', y='dim_3', hue=clust, palette='viridis', alpha=0.5, legend=True)\n",
    "sns.scatterplot(data=plot_df[plot_df.log_likelihood_gmm<0], x='dim_1', y='dim_3', color='black', alpha=0.1, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da1a1f9fee48aeb25414ae45b99f00113784206b411b315deabc951a0f32f6bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
